{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all warnings\n",
    "simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import sklearn algorithms\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "#useful text storage and processing imports\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "from nltk import corpus\n",
    "\n",
    "#might need to download if necessary\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"porter_test\")\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pStemmer = PorterStemmer()\n",
    "sStemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "LR 54.36142857142856\n",
      "CNB 58.19571428571429\n",
      "SVC 55.33142857142857\n",
      "DT 26.192857142857143\n",
      "MNB 57.261428571428574\n"
     ]
    }
   ],
   "source": [
    "#encoding our subreddits\n",
    "labels = {\n",
    "    \"anime\": 1,\n",
    "    \"AskReddit\": 2,\n",
    "    \"baseball\": 3,\n",
    "    \"canada\": 4, \n",
    "    \"conspiracy\": 5, \n",
    "    \"europe\": 6, \n",
    "    \"funny\": 7, \n",
    "    \"gameofthrones\": 8, \n",
    "    \"GlobalOffensive\": 9,\n",
    "    \"hockey\" :10, \n",
    "    \"leagueoflegends\": 11, \n",
    "    \"movies\": 12, \n",
    "    \"Music\": 13, \n",
    "    \"nba\":14, \n",
    "    \"nfl\":15, \n",
    "    \"Overwatch\":16, \n",
    "    \"soccer\":17, \n",
    "    \"trees\":18, \n",
    "    \"worldnews\":19, \n",
    "    \"wow\":20\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    #lowercase remove\n",
    "    lowercase_text = text.lower()\n",
    "    \n",
    "    #html, url, ampersand and line break remove\n",
    "    cleaned_text = re.sub('<[^<]+?>','', lowercase_text)\n",
    "    cleaned_text = re.sub(r'https?:\\/\\/.*[\\r\\n]*|www\\..*[\\r\\n]*', \"\", cleaned_text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'\\s&\\w+|&\\w+|&', \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\n', ' ', cleaned_text)\n",
    "    \n",
    "    #remove digits and punct.\n",
    "    no_dig = ''.join(c for c in cleaned_text if not c.isdigit())\n",
    "    pure_text = ''.join(c for c in no_dig if c not in punctuation)\n",
    "    \n",
    "    #remove stopwords and stemmer\n",
    "    word_tokens = nltk.word_tokenize(pure_text)\n",
    "    removing_stopwords = [word for word in word_tokens if word not in stop_words]\n",
    "    removing_stopwords = ' '.join(word for word in removing_stopwords)\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(removing_stopwords)\n",
    "    stemmed_word = [sStemmer.stem(word) for word in word_tokens]\n",
    "    stemmed_word = ' '.join(word for word in stemmed_word)\n",
    "    \n",
    "    #word_tokens = nltk.word_tokenize(removing_stopwords)\n",
    "    #lemmatized_word = [pStemmer.stem(word) for word in word_tokens]\n",
    "    #lemmatized_word = ' '.join(word for word in lemmatized_word)\n",
    "    \n",
    "    return stemmed_word\n",
    "\n",
    "def k_fold(dataframe, k):\n",
    "    temp = []\n",
    "    folds = []\n",
    "    r = dataframe.shape[0]\n",
    "    q = int(r/k)\n",
    "    rest = r%k\n",
    "\n",
    "    n = 0\n",
    "    for i in range(k):\n",
    "        if i < rest:\n",
    "            q += 1\n",
    "        data = dataframe.iloc[n:n+q]\n",
    "        temp.append(data)\n",
    "        n += q\n",
    "        if i < rest:\n",
    "            q -= 1\n",
    "\n",
    "    for i in range(k):\n",
    "        data = temp[i]\n",
    "        for j in range(1,k-1):\n",
    "            data = data.append(temp[(j+i)%k], ignore_index=True)\n",
    "        folds.append(data)\n",
    "        folds.append(temp[(i+k-1)%k])\n",
    "    return folds\n",
    "\n",
    "def main(classes):\n",
    "    train_data = pd.read_csv(\"reddit_train.csv\")\n",
    "    train_data = train_data.drop_duplicates(subset={\"id\"})\n",
    "    train_data = train_data.drop(['id'], axis=1)\n",
    "    train_data['subreddits'] = train_data['subreddits'].map(classes)\n",
    "    \n",
    "    #train_data['comments'] = train_data['comments'].apply(clean_text)\n",
    "    #train_data.to_csv(\"reddit_train_clean.csv\")\n",
    "    \n",
    "    k = 5\n",
    "    k_folds = k_fold(train_data, k)\n",
    "        \n",
    "    k = 0\n",
    "    scoreLR = 0\n",
    "    scoreCB = 0\n",
    "    scoreSVC = 0\n",
    "    scoreDT = 0\n",
    "    scoreMNB = 0\n",
    "    while (k<9):   \n",
    "        X_train = k_folds[k]['comments']\n",
    "        X_test = k_folds[k+1]['comments']\n",
    "        y_train = k_folds[k]['subreddits']\n",
    "        y_test = k_folds[k+1]['subreddits']\n",
    "\n",
    "        tf_idf_vectorizer = TfidfVectorizer(smooth_idf=True, sublinear_tf=True, norm='l2', max_df=0.42, strip_accents='unicode')\n",
    "        vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "        vectors_test_idf = tf_idf_vectorizer.transform(X_test)\n",
    "\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(vectors_train_idf, y_train)\n",
    "        y_pred = clf.predict(vectors_test_idf)\n",
    "        scoreLR = scoreLR + metrics.accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        clf = ComplementNB(alpha = 0.81)\n",
    "        clf.fit(vectors_train_idf, y_train)\n",
    "        y_pred = clf.predict(vectors_test_idf)\n",
    "        scoreCB = scoreCB + metrics.accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        clf = MultinomialNB(alpha = 0.17)\n",
    "        clf.fit(vectors_train_idf, y_train)\n",
    "        y_pred = clf.predict(vectors_test_idf)\n",
    "        scoreMNB = scoreMNB + metrics.accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        clf = LinearSVC()\n",
    "        clf.fit(vectors_train_idf, y_train)\n",
    "        y_pred = clf.predict(vectors_test_idf)\n",
    "        scoreSVC = scoreSVC + metrics.accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        #Decision trees are horrible for text classification\n",
    "        clf = DecisionTreeClassifier()\n",
    "        clf.fit(vectors_train_idf, y_train)\n",
    "        y_pred = clf.predict(vectors_test_idf)\n",
    "        scoreDT = scoreDT + metrics.accuracy_score(y_test, y_pred)\n",
    "        print(\"-\")\n",
    "        k = k+2\n",
    "    \n",
    "    print(\"LR\", 100*scoreLR/5)\n",
    "    print(\"CNB\", 100*scoreCB/5)\n",
    "    print(\"SVC\", 100*scoreSVC/5)\n",
    "    print(\"DT\", 100*scoreDT/5)\n",
    "    print(\"MNB\", 100*scoreMNB/5)\n",
    "\n",
    "main(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(num):\n",
    "    if num == 1:\n",
    "        return \"anime\"\n",
    "    elif num == 2:\n",
    "        return \"AskReddit\"\n",
    "    elif num == 3:\n",
    "        return \"baseball\"\n",
    "    elif num == 4:\n",
    "        return \"canada\"\n",
    "    elif num == 5:\n",
    "        return \"conspiracy\"\n",
    "    elif num == 6:\n",
    "        return \"europe\"\n",
    "    elif num == 7:\n",
    "        return \"funny\"\n",
    "    elif num == 8:\n",
    "        return \"gameofthrones\"\n",
    "    elif num == 9:\n",
    "        return \"GlobalOffensive\"\n",
    "    elif num == 10:\n",
    "        return \"hockey\"\n",
    "    elif num == 11:\n",
    "        return \"leagueoflegends\"\n",
    "    elif num == 12:\n",
    "        return \"movies\"\n",
    "    elif num == 13:\n",
    "        return \"Music\"\n",
    "    elif num == 14:\n",
    "        return \"nba\"\n",
    "    elif num == 15:\n",
    "        return \"nfl\"\n",
    "    elif num == 16:\n",
    "        return \"Overwatch\"\n",
    "    elif num == 17:\n",
    "        return \"soccer\"\n",
    "    elif num == 18:\n",
    "        return \"trees\"\n",
    "    elif num == 19:\n",
    "        return \"worldnews\"\n",
    "    elif num == 20:\n",
    "        return \"wow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"reddit_train.csv\")\n",
    "test_data = pd.read_csv(\"reddit_test.csv\")\n",
    "train_data = train_data.drop_duplicates(subset={\"id\"})\n",
    "train_data = train_data.drop(['id'], axis=1)\n",
    "test_data = test_data.drop(['id'], axis=1)\n",
    "train_data['subreddits'] = train_data['subreddits'].map(labels)\n",
    "\n",
    "X_train = train_data['comments']\n",
    "X_test = test_data['comments']\n",
    "y_train = train_data['subreddits']\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(smooth_idf=True, sublinear_tf=True, norm='l2', max_df=0.42, strip_accents='unicode')\n",
    "vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "vectors_test_idf = tf_idf_vectorizer.transform(X_test)\n",
    "\n",
    "clf = ComplementNB(alpha = 0.81)\n",
    "clf.fit(vectors_train_idf, y_train)\n",
    "y_pred = clf.predict(vectors_test_idf)\n",
    "shape = int(y_pred.shape[0])\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "\n",
    "i=0\n",
    "while (i < shape):\n",
    "    y_pred[0][i] = decode_labels(y_pred[0][i])\n",
    "    i = i + 1\n",
    "prediction = y_pred.to_csv('prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
